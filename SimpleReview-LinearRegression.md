# 복습

### <br/>선형 회귀 Linear Regression

<br/>

__1) 단순 선형 회귀 분석 (Simple Linear Regression Analysis)__  

![equation](https://latex.codecogs.com/svg.latex?y%20=%20Wx%20+b)

W : 가중치(weight)   

b : 편향(bias)   

<br/>

__2) 다중 선형 회귀 분석 (Multiple Linear Regression Analysis)__     

   

![equation](https://latex.codecogs.com/svg.latex?y%20=%20{W_1x_1%20+%20W_2x_2%20+%20...%20W_nx_n%20+%20b})       

y is still one   

Not one x, but several x      

<br/>

![equation](https://latex.codecogs.com/svg.latex?H(x)%20=%20{Wx%20+%20b})    

__H__ means Hypothesis    

<br/>

##### What we have to do by LR is to find _appropriate W and b_

<br/>

In ML, to find __W__ and __b__, we use __Cost function(Loss function)__    

Cost function is about the error between y and y^    

### We have to minimize Cost function!     



